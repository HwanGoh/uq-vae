#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Nov 23 10:53:31 2019

@author: hwan
"""
import sys
sys.path.append('../..')

import shutil # for deleting directories
import os
import time

import tensorflow as tf
import dolfin as dl
dl.set_log_level(30)
import numpy as np
import pandas as pd
from Thermal_Fin_Heat_Simulator.Utilities.forward_solve import Fin
from Thermal_Fin_Heat_Simulator.Utilities.thermal_fin import get_space_2D, get_space_3D

import pdb #Equivalent of keyboard in MATLAB, just add "pdb.set_trace()"

###############################################################################
#                             Training Properties                             #
###############################################################################
def optimize(hyperp, run_options, file_paths, NN, obs_indices, loss_autoencoder, loss_encoder_or_decoder, loss_forward_model, relative_error, reg_prior, L_pr, data_and_latent_train, data_and_latent_val, data_and_latent_test, data_dimension, num_batches_train):
    #=== Generate Dolfin Function Space and Mesh ===# These are in the scope and used below in the Fenics forward function and gradient
    if run_options.fin_dimensions_2D == 1:
        V, mesh = get_space_2D(40)
    if run_options.fin_dimensions_3D == 1:    
        V, mesh = get_space_3D(40)
    solver = Fin(V)
    parameter_pred_dl = dl.Function(V)
    print(V.dim())  
    if run_options.data_thermal_fin_nine == 1:
        B_obs = solver.observation_operator()  
    else:
        B_obs = np.zeros((len(obs_indices), V.dim()), dtype=np.float32)
        B_obs[np.arange(len(obs_indices)), obs_indices.flatten()] = 1
    
    #=== Optimizer ===#
    optimizer = tf.keras.optimizers.Adam()

    #=== Define Metrics ===#
    mean_loss_train = tf.keras.metrics.Mean()
    mean_loss_train_autoencoder = tf.keras.metrics.Mean() 
    mean_loss_train_encoder = tf.keras.metrics.Mean()
    mean_loss_train_decoder = tf.keras.metrics.Mean()
    mean_loss_train_forward_model = tf.keras.metrics.Mean()
    
    mean_loss_val = tf.keras.metrics.Mean()
    mean_loss_val_autoencoder = tf.keras.metrics.Mean()
    mean_loss_val_encoder = tf.keras.metrics.Mean()
    mean_loss_val_decoder = tf.keras.metrics.Mean()
    mean_loss_val_forward_model = tf.keras.metrics.Mean()
    
    mean_loss_test = tf.keras.metrics.Mean()
    mean_loss_test_autoencoder = tf.keras.metrics.Mean()
    mean_loss_test_encoder = tf.keras.metrics.Mean()
    mean_loss_test_decoder = tf.keras.metrics.Mean()
    mean_loss_test_forward_model = tf.keras.metrics.Mean()    
    
    mean_relative_error_data_autoencoder = tf.keras.metrics.Mean()
    mean_relative_error_latent_encoder = tf.keras.metrics.Mean()
    mean_relative_error_data_decoder = tf.keras.metrics.Mean()
    
    #=== Initialize Metric Storage Arrays ===#
    storage_array_loss_train = np.array([])
    storage_array_loss_train_autoencoder = np.array([])
    storage_array_loss_train_encoder = np.array([])
    storage_array_loss_train_decoder = np.array([])
    storage_array_loss_train_forward_model = np.array([])
    
    storage_array_loss_val = np.array([])
    storage_array_loss_val_autoencoder = np.array([])
    storage_array_loss_val_encoder = np.array([])
    storage_array_loss_val_decoder = np.array([])
    storage_array_loss_val_forward_model = np.array([])
    
    storage_array_loss_test = np.array([])
    storage_array_loss_test_autoencoder = np.array([])
    storage_array_loss_test_encoder = np.array([])
    storage_array_loss_test_decoder = np.array([])
    storage_array_loss_test_forward_model = np.array([])
    
    storage_array_relative_error_data_autoencoder = np.array([])
    storage_array_relative_error_latent_encoder = np.array([])
    storage_array_relative_error_data_decoder = np.array([])
    
    storage_array_relative_gradient = np.array([])

    #=== Creating Directory for Trained Neural Network ===#
    if not os.path.exists(file_paths.NN_savefile_directory):
        os.makedirs(file_paths.NN_savefile_directory)
    
    #=== Tensorboard ===# Tensorboard: type "tensorboard --logdir=Tensorboard" into terminal and click the link
    if os.path.exists(file_paths.tensorboard_directory): # Remove existing directory because Tensorboard graphs mess up of you write over it
        shutil.rmtree(file_paths.tensorboard_directory)  
    summary_writer = tf.summary.create_file_writer(file_paths.tensorboard_directory)

###############################################################################
#                     Fenics Forward Functions and Gradient                   #
###############################################################################
    @tf.custom_gradient
    def fenics_forward(parameter_pred):
        fenics_state_pred = np.zeros((parameter_pred.shape[0], len(obs_indices)))
        for m in range(parameter_pred.shape[0]):
            parameter_pred_dl.vector().set_local(parameter_pred[m,:].numpy())
            state_dl, _ = solver.forward(parameter_pred_dl)  
            state_data_values = state_dl.vector().get_local()
            if hyperp.data_type == 'full':
                fenics_state_pred[m,:] = state_data_values
            if hyperp.data_type == 'bnd':
                fenics_state_pred[m,:] = state_data_values[obs_indices].flatten()
        def fenics_forward_grad(dy):
            fenics_forward_grad = np.zeros((parameter_pred.shape[0], parameter_pred.shape[1]))
            for m in range(parameter_pred.shape[0]):
                Jac_forward = solver.sensitivity(parameter_pred_dl, B_obs)
                fenics_forward_grad[m,:] = tf.linalg.matmul(tf.expand_dims(dy[m,:],0), Jac_forward)
            return fenics_forward_grad
        return fenics_state_pred, fenics_forward_grad

###############################################################################
#                   Training, Validation and Testing Step                     #
###############################################################################
    #=== Train Step ===# NOTE: NOT YET CODED FOR REVERSE AUTOENCODER. Becareful of the logs and exp
    #@tf.function
    def train_step(batch_data_train, batch_latent_train):
        with tf.GradientTape() as tape:
            if file_paths.autoencoder_type == 'rev_':
                batch_state_obs_train = batch_data_train
                batch_parameter_pred = NN.encoder(batch_data_train)
            else:
                batch_data_pred_train_AE = NN(tf.math.log(batch_data_train))
                batch_latent_pred_train = NN.encoder(batch_data_train)
                batch_data_pred_train = NN.decoder(batch_latent_train)
                batch_loss_train_autoencoder = loss_autoencoder(batch_data_pred_train_AE, tf.math.log(batch_data_train))                    
                batch_loss_train_encoder = loss_encoder_or_decoder(batch_latent_pred_train, batch_latent_train, hyperp.penalty_encoder)
                batch_loss_train_decoder = loss_encoder_or_decoder(batch_data_pred_train, batch_data_train, hyperp.penalty_decoder)
                batch_loss_train_forward_model = loss_forward_model(hyperp, run_options, V, solver, obs_indices, fenics_forward, batch_latent_train, tf.math.exp(batch_data_pred_train_AE), hyperp.penalty_aug)
            batch_loss_train = batch_loss_train_autoencoder + batch_loss_train_encoder + batch_loss_train_decoder + batch_loss_train_forward_model
        gradients = tape.gradient(batch_loss_train, NN.trainable_variables)
        optimizer.apply_gradients(zip(gradients, NN.trainable_variables))
        mean_loss_train(batch_loss_train)
        mean_loss_train_autoencoder(batch_loss_train_autoencoder)
        mean_loss_train_encoder(batch_loss_train_encoder)
        mean_loss_train_decoder(batch_loss_train_decoder)
        mean_loss_train_forward_model(batch_loss_train_forward_model)
        return gradients

    #=== Validation Step ===#
    #@tf.function
    def val_step(batch_data_val, batch_latent_val):
        if file_paths.autoencoder_type == 'rev_':
            batch_state_obs_val = batch_data_val
            batch_parameter_pred = NN.encoder(batch_data_val)
        else:
            batch_data_pred_val_AE = NN(tf.math.log(batch_data_val))
            batch_latent_pred_val = NN.encoder(batch_data_val)
            batch_data_pred_val = NN.decoder(batch_latent_val)
            batch_loss_val_autoencoder = loss_autoencoder(batch_data_pred_val_AE, tf.math.log(batch_data_val))                    
            batch_loss_val_encoder = loss_encoder_or_decoder(batch_latent_pred_val, batch_latent_val, hyperp.penalty_encoder)
            batch_loss_val_decoder = loss_encoder_or_decoder(batch_data_pred_val, batch_data_val, hyperp.penalty_decoder)
            batch_loss_val_forward_model = loss_forward_model(hyperp, run_options, V, solver, obs_indices, fenics_forward, batch_latent_val, tf.math.exp(batch_data_pred_val_AE), hyperp.penalty_aug)
        batch_loss_val = batch_loss_val_autoencoder + batch_loss_val_encoder + batch_loss_val_decoder + batch_loss_val_forward_model
        mean_loss_val_autoencoder(batch_loss_val_autoencoder)
        mean_loss_val_encoder(batch_loss_val_encoder)
        mean_loss_val_decoder(batch_loss_val_decoder)
        mean_loss_val_forward_model(batch_loss_val_forward_model)
        mean_loss_val(batch_loss_val)     
    
    #=== Test Step ===#
    #@tf.function
    def test_step(batch_data_test, batch_latent_test):
        if file_paths.autoencoder_type == 'rev_':
            batch_state_obs_test = batch_data_test
            batch_parameter_pred = NN.encoder(batch_data_test)
        else:
            batch_data_pred_test_AE = NN(tf.math.log(batch_data_test))
            batch_latent_pred_test = NN.encoder(tf.math.log(batch_data_test))
            batch_data_pred_test_decoder = NN.decoder(batch_latent_test)
            
            batch_loss_test_autoencoder = loss_autoencoder(batch_data_pred_test_AE, tf.math.log(batch_data_test))                    
            batch_loss_test_encoder = loss_encoder_or_decoder(batch_latent_pred_test, batch_latent_test, hyperp.penalty_encoder)
            batch_loss_test_decoder = loss_encoder_or_decoder(batch_data_pred_test_decoder, batch_data_test, hyperp.penalty_decoder)
            batch_loss_test_forward_model = loss_forward_model(hyperp, run_options, V, solver, obs_indices, fenics_forward, batch_latent_test, tf.math.exp(batch_data_pred_test_AE), hyperp.penalty_aug)

            mean_relative_error_data_autoencoder(relative_error(batch_data_pred_test_AE, batch_data_test))
            mean_relative_error_latent_encoder(relative_error(batch_latent_pred_test, batch_latent_test))
            mean_relative_error_data_decoder(relative_error(batch_data_pred_test_decoder, batch_data_test))
        batch_loss_test = batch_loss_test_autoencoder + batch_loss_test_encoder + batch_loss_test_decoder + batch_loss_test_forward_model
        mean_loss_test_autoencoder(batch_loss_test_autoencoder)
        mean_loss_test_encoder(batch_loss_test_encoder)
        mean_loss_test_decoder(batch_loss_test_decoder)
        mean_loss_test_forward_model(batch_loss_test_forward_model)
        mean_loss_test(batch_loss_test)
        
###############################################################################
#                             Train Neural Network                            #
############################################################################### 
    print('Beginning Training')
    for epoch in range(hyperp.num_epochs):
        print('================================')
        print('            Epoch %d            ' %(epoch))
        print('================================')
        print(file_paths.filename)
        print('GPU: ' + run_options.which_gpu + '\n')
        print('Optimizing %d batches of size %d:' %(num_batches_train, hyperp.batch_size))
        start_time_epoch = time.time()
        for batch_num, (batch_data_train, batch_latent_train) in data_and_latent_train.enumerate():
            start_time_batch = time.time()
            gradients = train_step(batch_data_train, batch_latent_train)
            elapsed_time_batch = time.time() - start_time_batch
            #=== Display Model Summary ===#
            if batch_num == 0 and epoch == 0:
                NN.summary()
            if batch_num  == 0:
                print('Time per Batch: %.4f' %(elapsed_time_batch))
        
        #=== Computing Relative Errors Validation ===#
        for batch_data_val, batch_latent_val in data_and_latent_val:
            val_step(batch_data_val, batch_latent_val)
            
        #=== Computing Relative Errors Test ===#
        for batch_data_test, batch_latent_test in data_and_latent_test:
            test_step(batch_data_test, batch_latent_test)

        #=== Track Training Metrics, Weights and Gradients ===#
        with summary_writer.as_default():
            tf.summary.scalar('loss_training', mean_loss_train.result(), step=epoch)
            tf.summary.scalar('loss_training_autoencoder', mean_loss_train_autoencoder.result(), step=epoch)
            tf.summary.scalar('loss_training_encoder', mean_loss_train_encoder.result(), step=epoch)
            tf.summary.scalar('loss_training_decoder', mean_loss_train_decoder.result(), step=epoch)
            tf.summary.scalar('loss_training_forward_model', mean_loss_train_forward_model.result(), step=epoch)
            tf.summary.scalar('loss_val', mean_loss_val.result(), step=epoch)
            tf.summary.scalar('loss_val_autoencoder', mean_loss_val_autoencoder.result(), step=epoch)
            tf.summary.scalar('loss_val_encoder', mean_loss_val_encoder.result(), step=epoch)
            tf.summary.scalar('loss_val_decoder', mean_loss_val_decoder.result(), step=epoch)
            tf.summary.scalar('loss_val_forward_model', mean_loss_val_forward_model.result(), step=epoch)
            tf.summary.scalar('loss_test', mean_loss_test.result(), step=epoch)
            tf.summary.scalar('loss_test_autoencoder', mean_loss_test_autoencoder.result(), step=epoch)
            tf.summary.scalar('loss_test_encoder', mean_loss_test_encoder.result(), step=epoch)
            tf.summary.scalar('loss_test_decoder', mean_loss_test_decoder.result(), step=epoch)
            tf.summary.scalar('loss_test_forward_model', mean_loss_test_forward_model.result(), step=epoch)
            tf.summary.scalar('relative_error_data_autoencoder', mean_relative_error_data_autoencoder.result(), step=epoch)
            tf.summary.scalar('relative_error_data_decoder', mean_relative_error_data_decoder.result(), step=epoch)
            tf.summary.scalar('relative_error_latent_encoder', mean_relative_error_latent_encoder.result(), step=epoch)
            for w in NN.weights:
                tf.summary.histogram(w.name, w, step=epoch)
            l2_norm = lambda t: tf.sqrt(tf.reduce_sum(tf.pow(t, 2)))
            sum_gradient_norms = 0.0
            for gradient, variable in zip(gradients, NN.trainable_variables):
                tf.summary.histogram("gradients_norm/" + variable.name, l2_norm(gradient), step = epoch)    
                sum_gradient_norms += l2_norm(gradient)
                if epoch == 0:
                    initial_sum_gradient_norms = sum_gradient_norms
            tf.summary.scalar('sum_gradient_norms', sum_gradient_norms, step=epoch)         
            relative_gradient = sum_gradient_norms/initial_sum_gradient_norms                  

        #=== Update Storage Arrays ===#
        storage_array_loss_train = np.append(storage_array_loss_train, mean_loss_train.result())
        storage_array_loss_train_autoencoder = np.append(storage_array_loss_train_autoencoder, mean_loss_train_autoencoder.result())
        storage_array_loss_train_encoder = np.append(storage_array_loss_train_encoder, mean_loss_train_encoder.result())
        storage_array_loss_train_decoder = np.append(storage_array_loss_train_decoder, mean_loss_train_decoder.result())
        storage_array_loss_train_forward_model = np.append(storage_array_loss_train_forward_model, mean_loss_train_forward_model.result())
        storage_array_loss_val = np.append(storage_array_loss_val, mean_loss_val.result())
        storage_array_loss_val_autoencoder = np.append(storage_array_loss_val_autoencoder, mean_loss_val_autoencoder.result())
        storage_array_loss_val_encoder = np.append(storage_array_loss_val_encoder, mean_loss_val_encoder.result())
        storage_array_loss_val_decoder = np.append(storage_array_loss_val_decoder, mean_loss_val_decoder.result())
        storage_array_loss_val_forward_model = np.append(storage_array_loss_val_forward_model, mean_loss_val_forward_model.result())
        storage_array_loss_test = np.append(storage_array_loss_test, mean_loss_test.result())
        storage_array_loss_test_autoencoder = np.append(storage_array_loss_test_autoencoder, mean_loss_test_autoencoder.result())
        storage_array_loss_test_encoder = np.append(storage_array_loss_test_encoder, mean_loss_test_encoder.result())
        storage_array_loss_test_decoder = np.append(storage_array_loss_test_decoder, mean_loss_test_decoder.result())
        storage_array_loss_test_forward_model = np.append(storage_array_loss_test_forward_model, mean_loss_test_forward_model.result())
        storage_array_relative_error_data_autoencoder = np.append(storage_array_relative_error_data_autoencoder, mean_relative_error_data_autoencoder.result())
        storage_array_relative_error_latent_encoder = np.append(storage_array_relative_error_latent_encoder, mean_relative_error_latent_encoder.result())
        storage_array_relative_error_data_decoder = np.append(storage_array_relative_error_data_decoder, mean_relative_error_data_decoder.result())
        storage_array_relative_gradients = np.append(storage_array_relative_gradients, relative_gradient) 

        #=== Display Epoch Iteration Information ===#
        elapsed_time_epoch = time.time() - start_time_epoch
        print('Time per Epoch: %.4f\n' %(elapsed_time_epoch))
        print('Train Loss: Full: %.3e, AE: %.3e, Encoder: %.3e, Decoder: %.3e, Aug: %.3e' %(mean_loss_train.result(), mean_loss_train_autoencoder.result(), mean_loss_train_encoder.result(), mean_loss_train_decoder.result(), mean_loss_train_forward_model.result()))
        print('Val Loss: Full: %.3e, AE: %.3e, Encoder: %.3e, Decoder: %.3e, Aug: %.3e' %(mean_loss_val.result(), mean_loss_val_autoencoder.result(), mean_loss_val_encoder.result(), mean_loss_val_decoder.result(), mean_loss_val_forward_model.result()))
        print('Test Loss: Full: %.3e, AE: %.3e, Encoder: %.3e, Decoder: %.3e, Aug: %.3e' %(mean_loss_test.result(), mean_loss_test_autoencoder.result(), mean_loss_test_encoder.result(), mean_loss_test_decoder.result(), mean_loss_test_forward_model.result()))
        print('Rel Errors: AE: %.3e, Encoder: %.3e, Decoder: %.3e' %(mean_relative_error_data_autoencoder.result(), mean_relative_error_latent_encoder.result(), mean_relative_error_data_decoder.result()))
        print('Relative Gradient Norm: %.4f\n' %(relative_gradient))
        start_time_epoch = time.time()
        
        #=== Resetting Metrics ===#
        mean_loss_train.reset_states()
        mean_loss_train_autoencoder.reset_states()
        mean_loss_train_encoder.reset_states()
        mean_loss_train_decoder.reset_states()
        mean_loss_train_forward_model.reset_states()
        mean_loss_val.reset_states()
        mean_loss_val_autoencoder.reset_states()
        mean_loss_val_encoder.reset_states()    
        mean_loss_val_decoder.reset_states()
        mean_loss_val_forward_model.reset_states()    
        mean_loss_test.reset_states()
        mean_loss_test_autoencoder.reset_states()
        mean_loss_test_encoder.reset_states()
        mean_loss_test_decoder.reset_states()
        mean_loss_test_forward_model.reset_states()
        mean_relative_error_data_autoencoder.reset_states()
        mean_relative_error_latent_encoder.reset_states()
        mean_relative_error_data_decoder.reset_states()
            
        #=== Save Current Model ===#
        if epoch % 5 == 0:
            NN.save_weights(file_paths.NN_savefile_name)
            metrics_dict = {}
            metrics_dict['loss_train'] = storage_array_loss_train
            metrics_dict['loss_train_autoencoder'] = storage_array_loss_train_autoencoder
            metrics_dict['loss_train_forward_model'] = storage_array_loss_train_forward_model
            metrics_dict['loss_val'] = storage_array_loss_val
            metrics_dict['loss_val_autoencoder'] = storage_array_loss_val_autoencoder
            metrics_dict['loss_val_forward_model'] = storage_array_loss_val_forward_model
            metrics_dict['relative_error_parameter_autoencoder'] = storage_array_relative_error_data_autoencoder
            metrics_dict['relative_error_state_obs'] = storage_array_relative_error_latent_encoder
            metrics_dict['relative_error_parameter_inverse_problem'] = storage_array_relative_error_data_decoder
            df_metrics = pd.DataFrame(metrics_dict)
            df_metrics.to_csv(file_paths.NN_savefile_name + "_metrics" + '.csv', index=False)
            print('Current Model and Metrics Saved') 
            
        #=== Gradient Norm Termination Condition ===#
        if relative_gradient < 1e-6:
            print('Gradient norm tolerance reached, breaking training loop')
            break
            
    #=== Save Final Model ===#
    NN.save_weights(file_paths.NN_savefile_name)
    print('Final Model Saved') 
    
    return storage_array_loss_train, storage_array_loss_train_autoencoder, storage_array_loss_train_encoder, storage_array_loss_train_decoder, storage_array_loss_train_forward_model, storage_array_loss_val, storage_array_loss_val_autoencoder, storage_array_loss_val_encoder, storage_array_loss_val_decoder, storage_array_loss_val_forward_model, storage_array_loss_test, storage_array_loss_test_autoencoder, storage_array_loss_test_encoder, storage_array_loss_test_decoder, storage_array_loss_test_forward_model, storage_array_relative_error_data_autoencoder, storage_array_relative_error_latent_encoder, storage_array_relative_error_data_decoder, storage_array_relative_gradients
